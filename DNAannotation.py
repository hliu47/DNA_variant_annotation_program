#########################################################
### Simple variant annotation tool
### Version 0.0.1
### By Haipeng Liu 
### hliu47@uic.edu
#########################################################


import pandas as pd
import numpy as np
import allel
import argparse
import subprocess
import sys
import os.path
import pickle
import requests
import json


def extract_most_deleterious_anno(row, num_ann_max):
    ann_order = pd.read_csv(anno_order_file, sep=' ')

    alt = row[:num_ann_max]
    anno = row[num_ann_max:]

    alt.index = range(0, len(alt))
    anno.index = range(0, len(anno))

    ann_all_alt = pd.DataFrame()
    alt_unique = alt.unique()

    for unique_alt in alt_unique:

        if unique_alt != '':
            anno_all = anno[alt == unique_alt]
            ann_order_all = pd.DataFrame()

            for ann_any in anno_all:
                if sum(ann_any == ann_order.Anno) > 0:
                    ann_any_order = ann_order[ann_order.Anno == ann_any]
                else:
                    ann_any_order = ann_order.iloc[ann_order.shape[0]-1]

                ann_order_all = ann_order_all.append(ann_any_order)

            small_ann = ann_order_all.sort_index(ascending=True).Anno.iloc[0]
            ann_unique_alt = [unique_alt, small_ann]

            ann_all_alt = ann_all_alt.append(ann_unique_alt)
            ann_all_alt.index = range(0, ann_all_alt.shape[0])

    return ann_all_alt.T


def run_snpeff(temp_out_name):
    snpeff_command = ['java', '-Xmx4g', '-jar', snpeff_path, \
                      '-ud', '0', \
                      # '-v', \
                      '-canon', '-noStats', \
                      ref_genome, vcf_file]

    temp_output = open(temp_out_name, 'w')
    subprocess.run(snpeff_command, stdout=temp_output)
    temp_output.close()


def get_max_num_ann(temp_out_name):
    num_ann_guess = 500
    callset = allel.vcf_to_dataframe(temp_out_name, fields='ANN', numbers={'ANN': num_ann_guess})
    num_ann = callset.apply(lambda x: sum(x != ''), axis=1)
    num_ann_max = num_ann.max()  # num_ann_max = 175

    return num_ann_max


def get_ann_from_output_snpeff(temp_out_name):
    callset = allel.read_vcf(temp_out_name, fields='ANN', transformers=allel.ANNTransformer(), \
                             numbers={'ANN': num_ann_max})

    df1 = pd.DataFrame(data=callset['variants/ANN_Allele'])
    df2 = pd.DataFrame(data=callset['variants/ANN_Annotation'])
    df3 = pd.concat((df1, df2), axis=1)
    df3.columns = range(0, df3.shape[1])

    return df3


def get_anno_total(anno_from_snpeff):
    anno_total = pd.DataFrame()

    pickle_dump = 'pickle_dump.temp'
    if not os.path.isfile(pickle_dump):
        print('Extracting most deleterious annotations generated by SnpEff')
        for index, row in anno_from_snpeff.iterrows():
            anno_row = extract_most_deleterious_anno(row, num_ann_max)
            anno_total = anno_total.append(anno_row)
        print('done')
        dump_file = open(pickle_dump, 'wb')
        pickle.dump(anno_total, dump_file, pickle.HIGHEST_PROTOCOL)
        dump_file.close()

    dump_file = open(pickle_dump, 'rb')
    anno_total = pickle.load(dump_file)

    a = ['Alt_' + str(i) for i in range(1, num_alt + 1)]
    b = ['Anno_' + str(i) for i in range(1, num_alt + 1)]
    c = list(range(0, num_alt * 2))
    c[::2] = a
    c[1::2] = b

    anno_total.columns = c
    anno_total.replace(np.nan, -1, inplace=True)
    anno_total.index = range(0, anno_total.shape[0])

    return anno_total


def get_num_alternate(vcf_file):
    num_alt = allel.read_vcf(vcf_file, fields='numalt')['variants/numalt'].max()

    return num_alt


def get_dp_ro_ao(temp_out_name):
    callset_dp_ro_ao = allel.vcf_to_dataframe(temp_out_name, fields=['DP', 'RO', 'AO'], alt_number=num_alt)
    callset_dp_ro_ao.index = range(0, callset_dp_ro_ao.shape[0])

    return callset_dp_ro_ao


def get_alt_ref_ratio(callset_dp_ro_ao):
    callset_ratio = pd.DataFrame()
    for i in range(0, num_alt):
        # print('run ratio: ', i)
        callset_ratio[i] = callset_dp_ro_ao.apply(lambda x: x[i + 2] / x[1], axis=1)
        # print('run ratio: ', i, ' done')
    # print('callset_ratio is done')
    callset_ratio.columns = ['RatioAR_Alt_' + str(i) for i in range(1, num_alt + 1)]
    callset_ratio.index = range(0, callset_ratio.shape[0])

    return callset_ratio


def combine_anno_and_callset(anno_total, callset_dp_ro_ao, callset_ratio, ExAC_variant_af, ExAC_variant_ordered_csqs):
    anno_and_callset = pd.concat([anno_total, callset_dp_ro_ao, callset_ratio, ExAC_variant_af, ExAC_variant_ordered_csqs], axis=1)

    return anno_and_callset


def combine_with_comma(row):
    a = []
    for i in range(0, len(row)):
        if row.iloc[i][0] != '-':
            a.append(True)
        else:
            a.append(False)

    b = ",".join(row[a])

    return b


def get_anno_good(anno_and_callset):
    anno_columns = pd.DataFrame()
    for i in range(1, num_alt + 1):
        Alt_i = 'Alt_' + str(i)
        Anno_i = 'Anno_' + str(i)
        AO_i = 'AO_' + str(i)
        RatioAR_Alt_i = 'RatioAR_Alt_' + str(i)

        exac_var_af = 'exac_' + search_af + "_" + str(i)
        exac_ordered_csqs = 'exac_' + search_ordered_csqs + '_' + str(i)

        column_i = anno_and_callset[[Alt_i, Anno_i, 'DP', 'RO', AO_i, RatioAR_Alt_i, exac_var_af, exac_ordered_csqs]].apply(lambda x: '|'.join(x.map(str)), axis=1)
        anno_columns = pd.concat([anno_columns, column_i], axis=1)

    anno_one_column = anno_columns.apply(combine_with_comma, axis=1)

    anno_good = ["ANN="] * len(anno_one_column) + anno_one_column

    return anno_good


def get_num_lines_header(contents):
    lines_header = 0
    for i in range(0, len(contents)):
        if contents[i][0] == '#' and contents[i + 1][0] != '#':
            #         print(contents[i])
            #         print(i)
            lines_header = i  # lines_header 142

    return lines_header


def generate_output_vcf(vcf_file, anno_good):
    input_vcf = pd.read_csv(vcf_file, sep='\t', skiprows=lines_header)
    anno_good_all = input_vcf.INFO + ';' + anno_good
    input_vcf.INFO = anno_good_all
    output_vcf = input_vcf.copy()

    return output_vcf


def generate_header(contents):
    header = contents[0:lines_header]

    header_add1 = """##SimpleAnnotation Version="0.0.1" By Haipeng Liu hliu47@uic.edu \n"""
    header_add2 = """##SimpleAnnotation Cmd="python3 SimpleAnnotation.py -input {} -snpeff {} -genome {} "\n""".format(vcf_file, snpeff_path, ref_genome)
    header_add3 = """##INFO=<ID=ANN,Number=.,Type=String, Description="Simple annotations: 'Alternate allele | Type of variation most deleterious | Sequence depth at the site of variation | Number of reads of reference | Number of reads of alternate | Ratio of read counts of alt vs ref | ExAC variant Allele Frequency | ExAC variant consequence most deleterious' ">\n"""

    header.append(header_add1)
    header.append(header_add2)
    header.append(header_add3)

    return header


def search_REST_ExAC(row, search_type):
    row_var = [-1] * len(row)
    url_1 = 'http://exac.hms.harvard.edu/rest/variant/{}/'.format(search_type)
    for i in range(0, len(row)):
        if row.iloc[i][-1] != '-':
            url = url_1 + row.iloc[i]
            my_response = requests.get(url)

            if my_response.ok:
                j_data = json.loads(my_response.content)
                if search_type == search_af:
                    if 'allele_freq' in j_data.keys():
                        row_var[i] = j_data['allele_freq']
                    else:
                        row_var[i] = 'Not_found'

                elif search_type == search_ordered_csqs:
                    if j_data != None and len(j_data) > 1:
                        row_var[i] = j_data[1]
                    else:
                        row_var[i] = 'Not_found'

            else:
                row_var[i] = 'Not_found'

    return row_var


def ExAC_search_variant(var_all, search_type):
    exac = pd.DataFrame()
    counter = 0
    print('There are {} variants that need to be searched. This will take a while.'.format(var_all.shape[0]))
    for index, row in var_all.iterrows():
        af_row = search_REST_ExAC(row, search_type)
        exac = pd.concat([exac, pd.DataFrame(af_row)], axis=1)
        counter += 1
        if counter%500 == 0:
            print(counter)

    exac = exac.T
    exac.index = range(0, exac.shape[0])

    exac.columns = ['exac_' + search_type + '_' + str(i) for i in range(1, num_alt + 1)]

    return exac


def generate_var_id_for_exac(vcf_file):
    callset = allel.vcf_to_dataframe(vcf_file, fields=['CHROM', 'POS', 'REF', 'ALT'], alt_number=num_alt)
    var_all = pd.DataFrame()
    for i in range(1, num_alt+1):
        ALT_i = 'ALT_' + str(i)
        var_i = callset[['CHROM', 'POS', 'REF', ALT_i]].apply(lambda x: "-".join(x.map(str)), axis=1)
        var_all = pd.concat([var_all, var_i], axis=1)

    return var_all



# get input from command line
parser = argparse.ArgumentParser(description='Simple variant annotation tool ')
parser.add_argument('-input', help='input the VCF file with variant information')
parser.add_argument('-snpeff', help='path to snpEff.jar')
parser.add_argument('-genome', help='Genome version used in the input VCF file')

args = parser.parse_args()

vcf_file = args.input                           # Input VCF file
snpeff_path = args.snpeff                       # Path to the snpEff.jar of SnpEff
ref_genome = args.genome                        # Reference genome, obtained from input VCF file.
anno_order_file = 'ann_deleterious_order.txt'   # File that contains orders of variant deleteriousness according to Sequancy Ontology.
output_file = 'simple_annotation.vcf'           # Annotated VCF output from this program


# Run snpEff and save output
temp_out_name = 'temp_out_vcf.temp'
if not os.path.isfile(temp_out_name):
    print('Running SnpEff, this will take a while.')
    run_snpeff(temp_out_name)
    print('SnpEff done.')

# Read output of snpEff and get the maximum number of annotations in "ANN" field.
num_ann_max = get_max_num_ann(temp_out_name)

# Get annotations generated from SnpEff
anno_from_snpeff = get_ann_from_output_snpeff(temp_out_name)


# Get number of alternate allels.
num_alt = get_num_alternate(vcf_file)

# Process annotations and reformat.
anno_total = get_anno_total(anno_from_snpeff)

# Get DP, RO, and AO.
callset_dp_ro_ao = get_dp_ro_ao(temp_out_name)

# Compute ratio of AO to RO, this encounter warnings of divisions by zero, which provide inf as the result.
callset_ratio = get_alt_ref_ratio(callset_dp_ro_ao)

# Generate variant ID for search of ExAC.
var_all = generate_var_id_for_exac(vcf_file)

# Search variant allele frequency on ExAC.
print('Searching ExAC for allele freq.....')
search_af = 'variant'
ExAC_variant_af = ExAC_search_variant(var_all, search_af)
print('AF done')

# Search ordered consequences
print('Searching ordered consequences')
search_ordered_csqs = 'ordered_csqs'
ExAC_variant_ordered_csqs = ExAC_search_variant(var_all, search_ordered_csqs)
print('Ordered consequences search done.')

# Combine processed annotations and DP, RO, ratio of AO/RO and variant info from ExAC.
anno_and_callset = combine_anno_and_callset(anno_total, callset_dp_ro_ao, callset_ratio, ExAC_variant_af, ExAC_variant_ordered_csqs)

# Reformat annotations with "ANN" in the field.
anno_good = get_anno_good(anno_and_callset)

# Read input VCF file.
with open(vcf_file, 'r') as f_in:
    contents = f_in.readlines()

# Number of lines of header.
lines_header = get_num_lines_header(contents)

# Get annotations ready to write to output VCF.
output_vcf = generate_output_vcf(vcf_file, anno_good)

# Generated header of output VCF.
header = generate_header(contents)

# Write output VCF
with open(output_file, 'w') as f_out:
    f_out.writelines(header)
output_vcf.to_csv(output_file, index=False, sep='\t', mode='a')

print('All done, simple_annotation.vcf generated')

# Delete temporary files.
subprocess.run(['rm', 'temp_out_vcf.temp'])
subprocess.run(['rm', 'pickle_dump.temp'])
